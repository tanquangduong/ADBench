{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d806e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import the necessary package\n",
    "from data_generator import DataGenerator\n",
    "from myutils import Utils\n",
    "\n",
    "datagenerator = DataGenerator()\n",
    "utils = Utils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5bcded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline.PyOD import PYOD\n",
    "from baseline.Supervised import supervised\n",
    "from baseline.DAGMM.run import DAGMM # Unsup\n",
    "from baseline.DeepSAD.src.run import DeepSAD # Semi\n",
    "from baseline.REPEN.run import REPEN\n",
    "from baseline.DevNet.run import DevNet\n",
    "from baseline.PReNet.run import PReNet\n",
    "from baseline.FEAWAD.run import FEAWAD\n",
    "\n",
    "# dataset and model list / dict\n",
    "dataset_list = ['6_cardio.npz']\n",
    "model_dict = {'DeepSVDD':PYOD, 'DAGMM':DAGMM, 'COPOD': PYOD, 'ECOD': PYOD}\n",
    "\n",
    "# save the results\n",
    "df_AUCROC = pd.DataFrame(data=None, index=dataset_list, columns = model_dict.keys())\n",
    "df_AUCPR = pd.DataFrame(data=None, index=dataset_list, columns = model_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98dba74d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current noise type: None\n",
      "{'Samples': 1831, 'Features': 21, 'Anomalies': 176, 'Anomalies Ratio(%)': 9.61}\n",
      "************************* DeepSVDD\n",
      "best param: None\n",
      "WARNING:tensorflow:Output tf_op_layer_Sum missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to tf_op_layer_Sum.\n",
      "WARNING:tensorflow:Output tf_op_layer_Sum_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to tf_op_layer_Sum_1.\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 21)]              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                1344      \n",
      "                                                                 \n",
      " net_output (Dense)          (None, 32)                2048      \n",
      "                                                                 \n",
      " tf_op_layer_sub_1 (TensorFl  [(None, 32)]             0         \n",
      " owOpLayer)                                                      \n",
      "                                                                 \n",
      " tf_op_layer_pow_1 (TensorFl  [(None, 32)]             0         \n",
      " owOpLayer)                                                      \n",
      "                                                                 \n",
      " tf_op_layer_Sum_1 (TensorFl  [(None,)]                0         \n",
      " owOpLayer)                                                      \n",
      "                                                                 \n",
      " tf_op_layer_Mean_1 (TensorF  [()]                     0         \n",
      " lowOpLayer)                                                     \n",
      "                                                                 \n",
      " tf_op_layer_add_1 (TensorFl  [()]                     0         \n",
      " owOpLayer)                                                      \n",
      "                                                                 \n",
      " add_loss_1 (AddLoss)        ()                        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,392\n",
      "Trainable params: 3,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('Error when checking model target: expected no data, but got:', array([[ 0.33103447, -0.23026209, -0.2054641 , ...,  0.22376698,\n        -0.62831557, -0.51489416],\n       [ 0.43623297,  0.69637158, -0.2054641 , ...,  1.04373026,\n        -0.29046045, -0.51489416],\n       [-0.82614903,  1.59547714, -0.2054641 , ..., -0.45953576,\n        -0.45938801, -0.51489416],\n       ...,\n       [-0.61575203, -0.93031368, -0.2054641 , ..., -0.11788439,\n        -0.62831557, -0.51489416],\n       [-1.03654603,  0.08321484, -0.2054641 , ..., -0.73285685,\n        -0.56074455, -0.51489416],\n       [ 0.12063747, -0.93031368, -0.14054555, ..., -0.39120548,\n        -0.4256025 ,  1.12177934]]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     clf \u001b[38;5;241m=\u001b[39m clf(seed\u001b[38;5;241m=\u001b[39mseed, model_name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# training, for unsupervised models the y label will be discarded\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m clf \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# output predicted anomaly score on testing set\u001b[39;00m\n\u001b[0;32m     26\u001b[0m score \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict_score(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_test\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\PycharmProjects\\Anomaly-Detection\\ADBench\\baseline\\PyOD.py:298\u001b[0m, in \u001b[0;36mPYOD.fit\u001b[1;34m(self, X_train, y_train, ratio)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;66;03m# unsupervised method would ignore the y labels\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ad\\lib\\site-packages\\pyod\\models\\deep_svdd.py:276\u001b[0m, in \u001b[0;36mDeepSVDD.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;66;03m# Build DeepSVDD model & fit with X\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_model()\n\u001b[1;32m--> 276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhistory\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m# Predict on X itself and calculate the reconstruction error as\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# the outlier scores. Noted X_norm was shuffled has to recreate\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessing:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ad\\lib\\site-packages\\keras\\engine\\training_v1.py:776\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    775\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[1;32m--> 776\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ad\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py:616\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.fit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    596\u001b[0m         model,\n\u001b[0;32m    597\u001b[0m         x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    611\u001b[0m         validation_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    612\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    613\u001b[0m   batch_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_validate_or_infer_batch_size(batch_size,\n\u001b[0;32m    614\u001b[0m                                                    steps_per_epoch, x)\n\u001b[1;32m--> 616\u001b[0m   x, y, sample_weights \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize_user_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m      \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m      \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m      \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcheck_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msteps_per_epoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    628\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m validation_data:\n\u001b[0;32m    629\u001b[0m     val_x, val_y, val_sample_weights \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_prepare_validation_data(\n\u001b[0;32m    630\u001b[0m         validation_data, batch_size, validation_steps)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ad\\lib\\site-packages\\keras\\engine\\training_v1.py:2323\u001b[0m, in \u001b[0;36mModel._standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m run_eagerly \u001b[38;5;129;01mand\u001b[39;00m is_build_called \u001b[38;5;129;01mand\u001b[39;00m is_compile_called \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m   2320\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m is_dataset  \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(_is_symbolic_tensor(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m all_inputs)):\n\u001b[0;32m   2321\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [], [], \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize_tensors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_eagerly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_eagerly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdict_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdict_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ad\\lib\\site-packages\\keras\\engine\\training_v1.py:2408\u001b[0m, in \u001b[0;36mModel._standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2405\u001b[0m   feed_output_shapes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_output_shapes\n\u001b[0;32m   2407\u001b[0m \u001b[38;5;66;03m# Standardize the outputs.\u001b[39;00m\n\u001b[1;32m-> 2408\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_utils_v1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstandardize_input_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2409\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeed_output_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2411\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Don't enforce target shapes to match output shapes.\u001b[39;49;00m\n\u001b[0;32m   2412\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Precise checks will be run in `check_loss_and_target_compatibility`.\u001b[39;49;00m\n\u001b[0;32m   2413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Don't enforce the batch size.\u001b[39;49;00m\n\u001b[0;32m   2415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexception_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2417\u001b[0m \u001b[38;5;66;03m# Generate sample-wise weight values given the `sample_weight` and\u001b[39;00m\n\u001b[0;32m   2418\u001b[0m \u001b[38;5;66;03m# `class_weight` arguments.\u001b[39;00m\n\u001b[0;32m   2419\u001b[0m sample_weights \u001b[38;5;241m=\u001b[39m training_utils_v1\u001b[38;5;241m.\u001b[39mstandardize_sample_weights(\n\u001b[0;32m   2420\u001b[0m     sample_weight, feed_output_names)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ad\\lib\\site-packages\\keras\\engine\\training_utils_v1.py:557\u001b[0m, in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m names:\n\u001b[0;32m    556\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_len \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    558\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError when checking model \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m exception_prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    559\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpected no data, but got:\u001b[39m\u001b[38;5;124m'\u001b[39m, data)\n\u001b[0;32m    560\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: ('Error when checking model target: expected no data, but got:', array([[ 0.33103447, -0.23026209, -0.2054641 , ...,  0.22376698,\n        -0.62831557, -0.51489416],\n       [ 0.43623297,  0.69637158, -0.2054641 , ...,  1.04373026,\n        -0.29046045, -0.51489416],\n       [-0.82614903,  1.59547714, -0.2054641 , ..., -0.45953576,\n        -0.45938801, -0.51489416],\n       ...,\n       [-0.61575203, -0.93031368, -0.2054641 , ..., -0.11788439,\n        -0.62831557, -0.51489416],\n       [-1.03654603,  0.08321484, -0.2054641 , ..., -0.73285685,\n        -0.56074455, -0.51489416],\n       [ 0.12063747, -0.93031368, -0.14054555, ..., -0.39120548,\n        -0.4256025 ,  1.12177934]]))"
     ]
    }
   ],
   "source": [
    "# seed for reproducible results\n",
    "seed = 42\n",
    "\n",
    "for dataset in dataset_list:\n",
    "    '''\n",
    "    la: ratio of labeled anomalies, from 0.0 to 1.0\n",
    "    realistic_synthetic_mode: types of synthetic anomalies, can be local, global, dependency or cluster\n",
    "    noise_type: inject data noises for testing model robustness, can be duplicated_anomalies, irrelevant_features or label_contamination\n",
    "    '''\n",
    "    \n",
    "    # import the dataset\n",
    "    datagenerator.dataset = dataset # specify the dataset name\n",
    "    data = datagenerator.generator(la=0.1, realistic_synthetic_mode=None, noise_type=None) # only 10% labeled anomalies are available\n",
    "    for name, clf in model_dict.items():\n",
    "        # model initialization\n",
    "        print(\"*************************\", name)\n",
    "        if name == 'DevNet':\n",
    "            clf = clf(seed=seed, model_name=name, save_suffix='test') # DevNet use early stopping to save the model parameter\n",
    "        else:\n",
    "            clf = clf(seed=seed, model_name=name)\n",
    "        \n",
    "        # training, for unsupervised models the y label will be discarded\n",
    "        clf = clf.fit(X_train=data['X_train'], y_train=data['y_train'])\n",
    "        \n",
    "        # output predicted anomaly score on testing set\n",
    "        score = clf.predict_score(data['X_test'])\n",
    "#         if name ==\"DAGMM\":\n",
    "#             score = clf.predict_score(data['X_train'], data['X_test'])\n",
    "#         else:\n",
    "#             score = clf.predict_score(data['X_test'])\n",
    "\n",
    "        # evaluation\n",
    "        result = utils.metric(y_true=data['y_test'], y_score=score)\n",
    "        \n",
    "        # save results\n",
    "        df_AUCROC.loc[dataset, name] = result['aucroc']\n",
    "        df_AUCPR.loc[dataset, name] = result['aucpr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7f62bc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COPOD</th>\n",
       "      <th>ECOD</th>\n",
       "      <th>DeepSVDD</th>\n",
       "      <th>DAGMM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6_cardio.npz</th>\n",
       "      <td>0.928363</td>\n",
       "      <td>0.942827</td>\n",
       "      <td>0.689495</td>\n",
       "      <td>0.704567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 COPOD      ECOD  DeepSVDD     DAGMM\n",
       "6_cardio.npz  0.928363  0.942827  0.689495  0.704567"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_AUCROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9751725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COPOD</th>\n",
       "      <th>ECOD</th>\n",
       "      <th>DeepSVDD</th>\n",
       "      <th>DAGMM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6_cardio.npz</th>\n",
       "      <td>0.604146</td>\n",
       "      <td>0.592825</td>\n",
       "      <td>0.323306</td>\n",
       "      <td>0.20108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 COPOD      ECOD  DeepSVDD    DAGMM\n",
       "6_cardio.npz  0.604146  0.592825  0.323306  0.20108"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_AUCPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d10f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing performance metrics \n",
    "metrics_values = [['Accuracy', accuracy_score(y_test, y_pred)],\n",
    "                 ['Precision', precision_score(y_test, y_pred)],\n",
    "                 ['Recall', recall_score(y_test, y_pred)],\n",
    "                 ['F1_score', f1_score(y_test, y_pred)]]\n",
    "metrics_values_df = pd.DataFrame(metrics_values, columns=['Metrics', 'Result'])\n",
    "print(metrics_values_df)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualization\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax)\n",
    "ax.set_xlabel('Predicted Values')\n",
    "ax.set_ylabel('Actual Values')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(['Normal', 'Fraud'])\n",
    "ax.yaxis.set_ticklabels(['Normal', 'Fraud'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
